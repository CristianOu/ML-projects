# Exercise week 4

This is your 4th mandatory exercise for the track "Advanced Machine Learning in Computer Vision".
The tasks will be published on 19th September, and must be completed until a week later, i.e. Thursday 26.09.2024, 08:15.
By that time you must fill out the checklist on the learnit page to indicate which tasks you volunteer to present.
You are very welcome to present incomplete solutions and describe what challenges you faced.

## Task 1: Theory - Mapping it out

The purpose of this task is for you to create your own connections and overview of methods.

List of Methods:

- Principal Component Analysis (PCA)
- Kernel PCA
- Independent Component Analysis (ICA)
- Canonical Correlation Analysis (CCA)
- [Improving Model Convergence](https://chatgpt.com/c/66e0160e-4808-8004-8182-4ff58ed4bb49)Linear Discriminant Analysis (LDA
- Multidimensional Scaling (MDS)
- Isomap
- Locally Linear Embedding (LLE)
- Spectral Embedding
- t-Distributed Stochastic Neighbor Embedding (t-SNE)
- Uniform Manifold Approximation and Projection (UMAP)

List of Properties:

- supervised (can use class labels)
- feasible for continuous and/or discrete data
- can it handle missing data?
- Does it use local information?
- Does it preserve local or global structure?
- Assumes linear relationship or can handle non-linear relationship
- the original data can be reconstructed
- robust to noise
- can handle different dimensions in input

'''Your task:'''
Select at least 6 methods, and 4 properties. Assign the properties to the methods. Mark areas where you are uncertain.
You are free to add methods and properties to the list.

## Task 2: From Theory to Practise

Familiarize yourself with equations.
In the reading material and in the lecture, there were plenty of equations.
This exercise is supposed to help you practise rewriting. and moving towards coding for the next task.

a. Show that the matrix $\mathbf{C}_N=\mathbf{I}_n -\frac{1}{N} \mathbf{1}_N \mathbf{1}_N^T$ performs centering of the data matrix $\mathbf{X}\in \mathbb{R}^{N\times D}$, when multiplied from the left, i.e. show that

$$ \left(\begin{array}{c}
\mathbf{x}_1^T - \overline{\mathbf{x}}^T\\ 
\vdots \\
\mathbf{x}_N^T-\overline{\mathbf{x}}^T
\end{array}\right) =\mathbf{C}_N \mathbf{X}, $$

where $\mathbf{x}_i^T$ are the rows of the data matrix. (Note a slight notation difference in the reading material in PML).
If you struggle with the general case, simplify to a matrix with 2 dimensions and 3 data points.

b. How can PCA be used to perform regression? Explain the steps. (If necessary revisit what the term regression means.)

c. When performing dimensionality reduction via PCA using sklearn.decomposition.PCA, how do you have to set the input parameters?

## Task 3: Implement you own PCA

If you have not already done it in the past, now is your time to practise and learn!
Re-implement your own PCA.
Remember [this additional file](pca.md) for step-by-step instructions.
Does it lead the same or different results than sklearn?

## Task 4: Programming PCA Face Model

The purpose of this task is to create a 3D face model from a set of 3D faces given as sets of 3D points in faces.mat. 
The data was generated using the [Candide3 face model](https://people.isy.liu.se/icg/ahlberg/papers/reports/R-2326.html) which you can you to generate your own data, if you feel like it. 
Please check the exercise folder, where you can find candide.wrl, and candide3.mat. You might find one of the two formats more convenient for the data generation. 

If you wish to repeat the PCA look into [this additional file](pca.md), and implement you own version.

Hint: To read mat-files in Python:
import scipy.io
mat = scipy.io.loadmat('file.mat')

a. Estimate a PCA for the data matrix $X\in\mathbb{R}^{D\times N}$ (Note this notation for the data matrix is transposed compared to a previous task.). You can use your own implementation (see above), but if you don't have one, use a builtin solution to proceed with the following tasks. Here, you are being provided with $N=40$ samples, i.e. 3D faces, stored columnwise. Each face has a data dimension of $D=339$, i.e. $113$ 3D points per 3D face.

b. Familiarize yourself with the data format, and the ordering of the different point dimensions.
Plot the mean face $\overline{\mathbf{x}}\in \mathbb{R}^{D\times 1}$, and the variants of the first three principal components $\mathbf{u}_i \in\mathbb{R}^{D\times 1}$, $i=1,2,3$ as faces, i.e.: remember to add the mean face to each $\mathbf{u}_i$.

c. Create 3 new faces, which are not part of the data by choosing a fixed number of components $M$, and vary the weights $\alpha_k$.
    

$$
\mathbb{R}^{D\times 1} \ni \mathbf{f} = \overline{\mathbf{x}}+\sum\limits_{k=1}^M \alpha_k \mathbf{u}_k .
$$

d. Approximate a new face shape. Choose one of your created faces from task c, and pretend it is unknown, i.e. pretend that the weights $\alpha_k$ are unknown. (This would be the case for every new face, which is not part of the training data or has not been generated by you.) Then estimate the coefficients of the model with a lower dimension than before, i.e. choose $K < M$ and solve the following equation system for $\mathbf{\alpha}$:

$$
$\mathbb{R}^{D\times 1} \ni \mathbf{f}_\text{new}-\overline{\mathbf{x}} = \mathbf{U} \mathbf{\alpha},
$$

  where $\mathbf{\alpha} = (\alpha_1,\ldots,\alpha_K)^T \in\mathbb{R}^K$, and $\mathbf{U}=[\mathbf{u_1},\ldots,\mathbf{u_K} ] \in \mathbb{R}^{D\times K}$.
  Compute the approximated face $\mathbf{f}_{\text{new}}\approx\widehat{\mathbf{f}}=\overline{\mathbf{x}} + \mathbf{U} \mathbf{\alpha}$.
    Visualise your result by plotting the true vs. estimated face. Compute the error as$||\widehat{\mathbf{f}}-\mathbf{f}||_2^2$.

e. How many components do you need to preserve at least 90% of the variance of the data? keyword: proportion of variances.

Hint for visualisation: The file triangles.mat contains connective information for the face data points. It contains an array with 184 rows, where each row contains the point indices (starting at 1) of points which should be connected by a triangle.

Hint for Task 4d: How to solve this equation system if $U$ is not square? In this case the equation system is overdetermined. Revisit the material for equation system and linear regression. You will find that you can use the [pseudo inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) of $U$, as follows:

$$
\mathbf{f}_\text{new}-\overline{\mathbf{x}} = \mathbf{U} \mathbf{\alpha}
$$

$$
\mathbf{U}^T (\mathbf{f}_\text{new}-\overline{\mathbf{x}}) = \mathbf{U}^T\mathbf{U} \mathbf{\alpha}
$$

$$
(\mathbf{U}^T\mathbf{U})^{-1}\mathbf{U}^T (\mathbf{f}_\text{new}-\overline{\mathbf{x}}) = \mathbf{\alpha}
$$
